{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary modules and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docai_module.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Natural Language API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cloud Natural Language API lets you extract entities from text, perform sentiment and syntactic analysis, and classify text into categories.  \n",
    "In this lab, you learn how to use the Natural Language API to analyze entities, sentiment, and syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language features\n",
    "The Natural Language API has several methods for performing analysis and annotation on your text. Each level of analysis provides valuable information for language understanding. These methods are listed below:\n",
    "\n",
    " - **Sentiment analysis** inspects the given text and identifies the prevailing emotional opinion within the text, especially to determine a writer's attitude as positive, negative, or neutral. Sentiment analysis is performed through the analyzeSentiment method.\n",
    "\n",
    " - **Entity analysis inspects** the given text for known entities (Proper nouns such as public figures, landmarks, and so on. Common nouns such as restaurant, stadium, and so on.) and returns information about those entities. Entity analysis is performed with the analyzeEntities method.\n",
    "\n",
    " - **Entity sentiment analysis** inspects the given text for known entities (proper nouns and common nouns), returns information about those entities, and identifies the prevailing emotional opinion of the entity within the text, especially to determine a writer's attitude toward the entity as positive, negative, or neutral. Entity analysis is performed with the analyzeEntitySentiment method.\n",
    "\n",
    " - **Syntactic analysis** extracts linguistic information, breaking up the given text into a series of sentences and tokens (generally, word boundaries), providing further analysis on those tokens. Syntactic Analysis is performed with the analyzeSyntax method.\n",
    "\n",
    " - **Content classification** analyzes text content and returns a content category for the content. Content classification is performed by using the classifyText method.\n",
    "\n",
    "Each API call also detects and returns the language, if a language is not specified by the caller in the initial request.\n",
    "\n",
    "Additionally, if you wish to perform several natural language operations on given text using only one API call, the annotateText request can also be used to perform sentiment analysis and entity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Entities\n",
    "\n",
    "This function receives a text and sends to the API to do the **Entity Extraction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_entities(text_content: str):\n",
    "    \"\"\"\n",
    "    Analyzing Entities in a String\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "    lang = \"pt\"\n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": lang}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "    \n",
    "    response = NLP_CLIENT.analyze_entities(request = {'document': document, 'encoding_type': encoding_type})\n",
    "\n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "\n",
    "        # Get entity type, e.g. PERSON, LOCATION, ADDRESS, NUMBER, et al\n",
    "        print(u\"Entity type: {}\".format(language_v1.Entity.Type(entity.type_).name))\n",
    "\n",
    "        # Get the salience score associated with the entity in the [0, 1.0] range\n",
    "        print(u\"Salience score: {}\".format(entity.salience))\n",
    "\n",
    "        # Loop over the metadata associated with entity. For many known entities,\n",
    "        # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "        # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "        # may have metadata for the address street_name, postal_code, et al.\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            print(u\"{}: {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "        # Loop over the mentions of this entity in the input document.\n",
    "        # The API currently supports proper noun mentions.\n",
    "        for mention in entity.mentions:\n",
    "            print(u\"Mention text: {}\".format(mention.text.content))\n",
    "\n",
    "            # Get the mention type, e.g. PROPER for proper noun\n",
    "            print(\n",
    "                u\"Mention type: {}\".format(language_v1.EntityMention.Type(mention.type_).name)\n",
    "            )\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representative name for the entity: Osasco\n",
      "Entity type: LOCATION\n",
      "Salience score: 0.6129672527313232\n",
      "wikipedia_url: https://fr.wikipedia.org/wiki/Microregion_d'Osasco\n",
      "mid: /g/122n5xvw\n",
      "Mention text: Osasco\n",
      "Mention type: PROPER\n",
      "Representative name for the entity: lugar\n",
      "Entity type: OTHER\n",
      "Salience score: 0.38703271746635437\n",
      "Mention text: lugar\n",
      "Mention type: COMMON\n",
      "Representative name for the entity: um\n",
      "Entity type: NUMBER\n",
      "Salience score: 0.0\n",
      "value: 1\n",
      "Mention text: um\n",
      "Mention type: TYPE_UNKNOWN\n",
      "Language of the text: pt\n"
     ]
    }
   ],
   "source": [
    "analyze_entities('Osasco, um lugar legal para se morar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text_content: str):\n",
    "    \"\"\"\n",
    "    Analyzing Sentiment in a String\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "    lang = \"pt\"\n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": lang}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    response = NLP_CLIENT.analyze_sentiment(request = {'document': document, 'encoding_type': encoding_type})\n",
    "    # Get overall sentiment of the input document\n",
    "    print(u\"Document sentiment score: {}\".format(response.document_sentiment.score))\n",
    "    print(\n",
    "        u\"Document sentiment magnitude: {}\".format(\n",
    "            response.document_sentiment.magnitude\n",
    "        )\n",
    "    )\n",
    "    # Get sentiment for all sentences in the document\n",
    "    for sentence in response.sentences:\n",
    "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document sentiment score: -0.6000000238418579\n",
      "Document sentiment magnitude: 0.6000000238418579\n",
      "Sentence text: hoje não é um bom dia para caminhar\n",
      "Sentence sentiment score: -0.6000000238418579\n",
      "Sentence sentiment magnitude: 0.6000000238418579\n",
      "Language of the text: pt\n"
     ]
    }
   ],
   "source": [
    "analyze_sentiment('hoje não é um bom dia para caminhar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: AutoML Natural Language\n",
    "\n",
    "AutoML Natural Language uses machine learning to analyze the structure and meaning of documents. You train a custom machine learning model to classify documents, extract information, or understand the sentiment of authors.\n",
    "\n",
    " - A **classification model** analyzes a document and returns a list of content categories that apply to the text found in the document.\n",
    "\n",
    " - An **entity extraction model** inspects a document for known entities referenced in the document and labels those entities in the text.\n",
    "\n",
    " - A **sentiment analysis model** inspects a document and identifies the prevailing emotional opinion within it, especially to determine a writer's attitude as positive, negative, or neutral.\n",
    "\n",
    "Basic classification, entity extraction, and sentiment analysis are available through the Cloud Natural Language API. AutoML Natural Language enables you to define custom classification categories, entities, and sentiment scores that are relevant to your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable the AutoML Natural Language API\n",
    "\n",
    "Visit the following link to enable the AutoML Natural Language API, just like you did for Natural Language API:\n",
    "\n",
    "https://console.cloud.google.com/flows/enableapi?apiid=storage-component.googleapis.com,automl.googleapis.com,storage-api.googleapis.com&redirect=https://console.cloud.google.com&_ga=2.56437803.1602512825.1610975711-1349947163.1610541600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Classification\n",
    "\n",
    "This tutorial demonstrates how to create a custom model for classifying content using AutoML Natural Language. The application trains a custom model using a corpus of crowd-sourced \"happy moments\" from the Kaggle open-source dataset HappyDB. The resulting model classifies happy moments into categories reflecting the causes of happiness.\n",
    "\n",
    "#### ATENTION!\n",
    "\n",
    "Importing and training a model in AutoML can take hours or days.  \n",
    "To simplify and speedup this process, the dataset was reduced to a very small sample. This means that the performance of the model **CAN'T** be taken in consideration, knowing the very few examples provided.\n",
    "\n",
    "Before creating the model in AutoML, let's explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./files/happiness.csv', 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went on a successful date with someone I felt sympathy and connection with.,affection\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we want to create a Classification model (single label) which takes as inputs a text and its label.  \n",
    "[AutoML expects this inputs like the following](https://cloud.google.com/natural-language/automl/docs/prepare):\n",
    "\n",
    "[TRAIN, VALIDATION, TEST],your text,[label]  \n",
    "Example: TRAIN, I really want to go sky., travel\n",
    "\n",
    "The first token, TRAIN, indicates that the text will be used in the training phase. If you don't include this token, AutoML will split your dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION_NAME='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a bucket in Google Cloud Storage to upload our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://cool-ml-demos-lcm/...\n"
     ]
    }
   ],
   "source": [
    "# Create a bucket in GCS to store the documents\n",
    "!gsutil mb -p $PROJECT_ID -c regional -l $REGION_NAME gs://$PROJECT_ID-lcm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-ml-data/NL-classification/happiness.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  1.3 MiB/  1.3 MiB]                                                \n",
      "Operation completed over 1 objects/1.3 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Copy the Happiness dataset to your bucket\n",
    "!gsutil cp ./files/happiness.csv gs://$PROJECT_ID-lcm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Create the dataset in AutoML\n",
    "\n",
    "Dataset is an empty container where we can upload our documents to later train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(project_id, display_name):\n",
    "    \"\"\"Create a dataset.\"\"\"\n",
    "    # [START automl_language_text_classification_create_dataset]\n",
    "    from google.cloud import automl\n",
    "\n",
    "    client = automl.AutoMlClient()\n",
    "\n",
    "    # A resource that represents Google Cloud Platform location.\n",
    "    project_location = f\"projects/{project_id}/locations/us-central1\"\n",
    "    # Specify the classification type\n",
    "    # Types:\n",
    "    # MultiLabel: Multiple labels are allowed for one example.\n",
    "    # MultiClass: At most one label is allowed per example.\n",
    "    metadata = automl.TextClassificationDatasetMetadata(\n",
    "        classification_type=automl.ClassificationType.MULTICLASS\n",
    "    )\n",
    "    dataset = automl.Dataset(\n",
    "        display_name=display_name,\n",
    "        text_classification_dataset_metadata=metadata,\n",
    "    )\n",
    "\n",
    "    # Create a dataset with the dataset metadata in the region.\n",
    "    response = client.create_dataset(parent=project_location, dataset=dataset)\n",
    "\n",
    "    created_dataset = response.result()\n",
    "\n",
    "    # Display the dataset information\n",
    "    print(\"Dataset name: {}\".format(created_dataset.name))\n",
    "    print(\"Dataset id: {}\".format(created_dataset.name.split(\"/\")[-1]))\n",
    "    \n",
    "    return created_dataset.name.split(\"/\")[-1]\n",
    "    # [END automl_language_text_classification_create_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: projects/411150075841/locations/us-central1/datasets/TCN3738903034147635200\n",
      "Dataset id: TCN3738903034147635200\n"
     ]
    }
   ],
   "source": [
    "dataset_id = create_dataset(project_id, 'happydb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify if the dataset was created, go to the Google Cloud Web console and navigate to:\n",
    " - AutoML Natural Language > Datasets\n",
    " \n",
    "<img src=\"./images/3_2_menu_automl.png\"\n",
    "     alt=\"AutoML Dataset\"\n",
    "     style=\"width:25%\"\n",
    "     />\n",
    "     \n",
    "You should see an entry with the dataset you just created:\n",
    "\n",
    "<img src=\"./images/3_3_newdataset.png\"\n",
    "     alt=\"AutoML Dataset\"\n",
    "     style=\"width:70%\"\n",
    "     />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Import the dataset\n",
    "\n",
    "The next step is to populate the dataset with a list of training content items labeled using the target categories.\n",
    "\n",
    "The import_dataset function interface takes as input a .csv file that lists the locations of all training documents and the proper label for each training document. (See Preparing your training data for details about the required format.) For this tutorial, we will be using happiness.csv, which you uploaded to Google Cloud Storage above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(project_id, dataset_id, path):\n",
    "    \"\"\"Import a dataset.\"\"\"\n",
    "    # [START automl_import_data]\n",
    "    from google.cloud import automl\n",
    "\n",
    "    client = automl.AutoMlClient()\n",
    "    # Get the full path of the dataset.\n",
    "    dataset_full_id = client.dataset_path(\n",
    "        project_id, \"us-central1\", dataset_id\n",
    "    )\n",
    "    # Get the multiple Google Cloud Storage URIs\n",
    "    input_uris = path.split(\",\")\n",
    "    gcs_source = automl.GcsSource(input_uris=input_uris)\n",
    "    input_config = automl.InputConfig(gcs_source=gcs_source)\n",
    "    # Import data from the input URI\n",
    "    response = client.import_data(name=dataset_full_id, input_config=input_config)\n",
    "\n",
    "    print(\"Processing import...\")\n",
    "    print(\"Data imported. {}\".format(response.result()))\n",
    "    # [END automl_import_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'gs://{project_id}-lcm/happiness.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing import...\n"
     ]
    }
   ],
   "source": [
    "# This process may take several minutes. \n",
    "import_dataset(project_id, dataset_id, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the import is finished, you should see the data in the AutoML NL console.\n",
    "\n",
    "You can see the quantity of itens uploaded for each specific label.\n",
    "It is possible to click on a specific item and change its label.\n",
    "\n",
    "<img src=\"./images/3_4_importdata.png\"\n",
    "     alt=\"AutoML Dataset\"\n",
    "     style=\"width:70%\"\n",
    "     />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Create the model\n",
    "Now that you have a dataset of labeled training documents, you can train a new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(project_id, dataset_id, display_name):\n",
    "    \"\"\"Create a model.\"\"\"\n",
    "    from google.cloud import automl\n",
    "\n",
    "    client = automl.AutoMlClient()\n",
    "\n",
    "    # A resource that represents Google Cloud Platform location.\n",
    "    project_location = f\"projects/{project_id}/locations/us-central1\"\n",
    "    # Leave model unset to use the default base model provided by Google\n",
    "    metadata = automl.TextClassificationModelMetadata()\n",
    "    model = automl.Model(\n",
    "        display_name=display_name,\n",
    "        dataset_id=dataset_id,\n",
    "        text_classification_model_metadata=metadata,\n",
    "    )\n",
    "\n",
    "    # Create a model with the model metadata in the region.\n",
    "    response = client.create_model(parent=project_location, model=model)\n",
    "\n",
    "    print(u\"Training operation name: {}\".format(response.operation.name))\n",
    "    print(\"Training started...\")\n",
    "\n",
    "    # Return the model_id\n",
    "    return response.operation.name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training operation name: projects/411150075841/locations/us-central1/operations/TCN2899544378718552064\n",
      "Training started...\n"
     ]
    }
   ],
   "source": [
    "model_id = create_model(project_id, dataset_id, 'mymodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the web console you should see the model creation progress:\n",
    "\n",
    "<img src=\"./images/3_5_modelcreate.png\"\n",
    "     alt=\"AutoML Dataset\"\n",
    "     style=\"width:30%\"\n",
    "     />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Evaluate the model\n",
    "\n",
    "After training, you can evaluate your model's readiness by reviewing its precision, recall, and F1 score.\n",
    "\n",
    "The display_evaluation function takes the Model ID as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_model_evaluations(project_id, model_id):\n",
    "    \"\"\"List model evaluations.\"\"\"\n",
    "    from google.cloud import automl\n",
    "\n",
    "    client = automl.AutoMlClient()\n",
    "    # Get the full path of the model.\n",
    "    model_full_id = client.model_path(project_id, \"us-central1\", model_id)\n",
    "\n",
    "    print(\"List of model evaluations:\")\n",
    "    for evaluation in client.list_model_evaluations(parent=model_full_id, filter=\"\"):\n",
    "        print(\"Model evaluation name: {}\".format(evaluation.name))\n",
    "        print(\n",
    "            \"Model annotation spec id: {}\".format(\n",
    "                evaluation.annotation_spec_id\n",
    "            )\n",
    "        )\n",
    "        print(\"Create Time: {}\".format(evaluation.create_time))\n",
    "        print(\n",
    "            \"Evaluation example count: {}\".format(\n",
    "                evaluation.evaluated_example_count\n",
    "            )\n",
    "        )\n",
    "        print(\n",
    "            \"Classification model evaluation metrics: {}\".format(\n",
    "                evaluation.classification_evaluation_metrics\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_model_evaluations(project_id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Deploy the model\n",
    "\n",
    "When your custom model meets your quality standards, you can deploy it and then make predictions request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(project_id, model_id):\n",
    "    \"\"\"Deploy a model.\"\"\"\n",
    "    from google.cloud import automl\n",
    "\n",
    "    client = automl.AutoMlClient()\n",
    "    model_full_id = client.model_path(project_id, \"us-central1\", model_id)\n",
    "    response = client.deploy_model(name=model_full_id)\n",
    "\n",
    "    print(f\"Model deployment finished. {response.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model(project_id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6 - Use the model to make a prediction\n",
    "\n",
    "After you deploy your model, you can use it to classify novel content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(project_id, model_id, content):\n",
    "    \"\"\"Predict.\"\"\"\n",
    "    from google.cloud import automl\n",
    "    prediction_client = automl.PredictionServiceClient()\n",
    "\n",
    "    # Get the full path of the model.\n",
    "    model_full_id = automl.AutoMlClient.model_path(\n",
    "        project_id, \"us-central1\", model_id\n",
    "    )\n",
    "\n",
    "    # Supported mime_types: 'text/plain', 'text/html'\n",
    "    # https://cloud.google.com/automl/docs/reference/rpc/google.cloud.automl.v1#textsnippet\n",
    "    text_snippet = automl.TextSnippet(\n",
    "        content=content, mime_type=\"text/plain\"\n",
    "    )\n",
    "    payload = automl.ExamplePayload(text_snippet=text_snippet)\n",
    "    response = prediction_client.predict(name=model_full_id, payload=payload)\n",
    "\n",
    "    for annotation_payload in response.payload:\n",
    "        print(\n",
    "            u\"Predicted class name: {}\".format(annotation_payload.display_name)\n",
    "        )\n",
    "        print(\n",
    "            u\"Predicted class score: {}\".format(\n",
    "                annotation_payload.classification.score\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(project_id, model_id, 'I finally passed the exams.')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "ml-demos3.8",
   "language": "python",
   "name": "ml-demos3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
